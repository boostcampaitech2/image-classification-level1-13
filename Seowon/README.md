> <i> ìƒˆë¡­ê²Œ ì‹œë„í•œ ì ì€ ğŸ”… ì´ê±¸ë¡œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. </i><br/>
> <i> ì¶”ê°€ë¡œ ì‹œë„í•´ì•¼ í•  ì ì€ âœ… ì´ê±¸ë¡œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤. </i>

<b> xception.ipynb </b>
  - ëª¨ë¸ : Xception
  - ì„±ëŠ¥ : acc 76.603, f1 0.687 ğŸ’¥
  - train, validation ë¹„ìœ¨ : 9:1
  - optimizer : Adam
  - scheduler : StepLR
  - error : test inference í•˜ëŠ” ê³¼ì •ì—ì„œ ì´ ì—ëŸ¬ ë‚¬ëŠ”ë°, https://stackoverflow.com/questions/57079219/img-should-be-pil-image-got-class-torch-tensor
            transformsì—ì„œ toTensor ë‹¤ìŒì— Normalize 
  - ğŸ”… batch size 64ë¡œ ëŠ˜ë ¤ì„œ í•´ë´¤ëŠ”ë° ì •í™•ë„ 75í”„ë¡œë¡œ ì¡°ê¸ˆ ë‚®ì•„ì§. Xceptionì€ batch 32ê°€ ì ë‹¹í•´ë³´ì„
  - ğŸ”… train : validation ë¹„ìœ¨ 95:5ë³´ë‹¤ 9:1ì´ ë” ì¢‹ìŒ. learning_rate 1e-4ë³´ë‹¤ 1e-3ì´ ë” ì¢‹ìŒ
  - dropout 0.7ë³´ë‹¤ dropout 0.5ê°€ ë” ì¢‹ìŒ. dropout 0.7 acc = 75.5873

<b> xception2.ipynb </b>
  - ëª¨ë¸ : Xception
  - ì„±ëŠ¥ : acc 71.46
  - train, val ë¹„ìœ¨ : 9:1
  - ğŸ”… optimizer : optim.RAdam(model.parameters(), lr=0.0015, betas=(0.9, 0.999), weight_decay=1e-4)
  - ğŸ”… scheduler : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)
  - ğŸ”… dropout : 0.5
  - âœ… optimizerì™€ schedulerì˜ lrê³¼ T_max ê°’ì„ ë°”ê¿”ì•¼í•  ê²ƒ ê°™ìŒ. íŠ¹íˆ T_max ê°’ì„ batch size í¬ê¸°ì™€ ë¹„ë¡€í•´ì„œ ì¡ì•„ì•¼í•˜ëŠ”ë° ë„ˆë¬´ í¬ê²Œ ì¡ì€ ê²ƒ ê°™ìŒ

 
   
<b> efficientnet.ipynb </b>
  - ëª¨ë¸ : efficientnet b0
  - ì„±ëŠ¥ : acc 73.5873, f1 0.6548
  - train, validation ë¹„ìœ¨ : 95:5
  - optimizer : Adam
  - scheduler : StepLR
  - ğŸ”… OOM ë¬¸ì œë¡œ ì´ë¯¸ì§€ì˜ ê°€ë¡œ, ì„¸ë¡œ ê¸¸ì´ë¥¼ ë°˜ìœ¼ë¡œ ì¤„ì„. ê·¼ë° ì„œë²„ ì¬ì‹œì‘í•˜ë‹ˆê¹Œ OOM ë¬¸ì œ ì—†ì–´ì¡Œìœ¼ë‹ˆê¹Œ ì›ë³¸ í¬ê¸°ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ê¸°
  - ğŸ”… epochì´ 3ì´ ë„˜ì–´ê°€ë©´ì„œ accuracyê°€ ê³„ì† 100ìœ¼ë¡œ ì¸¡ì •ë˜ê¸¸ë˜, overfittingì´ ë°œìƒí•œ ê²ƒ ê°™ì•„ì„œ epoch 2 ê¹Œì§€ë§Œ ëŒë¦¬ê³  ì œì¶œí•´ë´¤ëŠ”ë° acc 71%ê°€ ë‚˜ì˜´. dropout ë“±ì˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ overfittingì„ ì¡ê³ , early stop ë˜ëŠ” ê²½ìš°ë¥¼ ë” íƒ€ì´íŠ¸í•˜ê²Œ ì¡ì•„ì•¼ í•  ê²ƒ ê°™ìŒ
  - ğŸ”… efficientnet.ipynbì—ì„œ Resizeë¥¼ ì‘ê²Œ í•˜ì§€ ì•Šê³  ì›ë³¸ í¬ê¸°ë¡œ í•™ìŠµí•˜ê³  batchsizeë„ 64ë¡œ ëŠ˜ë ¸ë”ë‹ˆ <b>acc 75.6032, f1 0.6770</b> ë¡œ ìƒìŠ¹
  - ğŸ”… dropout 0.5 ì¶”ê°€í–ˆëŠ”ë° ğŸ’¥ 76.9841, 0.6925 ğŸ’¥ë¡œ ì„±ëŠ¥ í–¥ìƒ - [efficientnet_dropout.ipynb](https://github.com/boostcampaitech2/image-classification-level1-13/blob/main/Seowon/efficientnet_dropout.ipynb) 

<b> xception_multi_output directory </b>
  - ëª¨ë¸ : xception
  - ì„±ëŠ¥ : acc 70.5238%, f1 0.6019
  - train, validation ë¹„ìœ¨ : 9:1
  - optimizer : Adam
  - scheduler : StepLR
  - âœ… Xception timmìœ¼ë¡œ ê°€ì ¸ì˜¤ë‹ˆê¹Œ ì—ëŸ¬ë– ì„œ ì „ì²´ ì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í–ˆëŠ”ë°, pretrained ëª¨ë¸ì„ ì‚¬ìš© ëª»í•´ì„œ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ” ê²ƒ ê°™ê¸°ë„ í•¨ -> pretrained í•´ê²° í›„ ì •í™•ë„ 74ë¡œ ìƒìŠ¹
  - ğŸ”… train setì— ëŒ€í•œ accuracyëŠ” 99, 100ê¹Œì§€ ë‚˜ì˜¤ëŠ”ë° validation setì— ëŒ€í•œ accuracyëŠ” 77ì´ ìµœëŒ€ì¸ ê±¸ ë³´ë‹ˆ overfitting ë¬¸ì œê°€ ìˆì–´ì„œ, dropout 0.5ë¥¼ ì¶”ê°€í•¨
  - ğŸ”… weight_decayì— l2 normalizationì„ ì ìš©
  - ì°¸ê³  ì‚¬ì´íŠ¸ 
  : [multioutputê´€ë ¨1](https://medium.com/jdsc-tech-blog/multioutput-cnn-in-pytorch-c5f702d4915f) 
    [multioutputê´€ë ¨2](https://learnopencv.com/multi-label-image-classification-with-pytorch/)
    [feature/classifier](https://rwightman.github.io/pytorch-image-models/feature_extraction/#multi-scale-feature-maps-feature-pyramid)
    [xception code](https://github.com/tstandley/Xception-PyTorch/blob/master/xception.py)
  
  - ğŸ”… age ì˜ˆì¸¡ ë¶„í¬ ì¤‘ì— 30~59ê°€ ì ê¸°ë„ í•˜ê³ , lossë„ ë‚®ê²Œ ë‚˜ì™€ì„œ ageì˜ lossì— 1.2ë¥¼ ê³±í•´ì¤Œ ì„±ëŠ¥ í–¥ìƒ -> 76.5238, 0.6814
  

<b> basecode </b>
  - ëª¨ë¸ : xception
  - ì„±ëŠ¥ : (<i>f1 loss</i>) acc 72.5556% f1 0.6304
  - val_ratio : 0.2
  - optimizer : Adam
  - scheduler : CosineAnnealingLR
  - ğŸ”… loss functionì„ f1, label_smoothing, cross_entropy, focal loss ë¥¼ ì‚¬ìš©í•´ë´¤ëŠ”ë°, cross_entropy > label_smoothing > f1 ìˆœìœ¼ë¡œ ì¢‹ìŒ
  - ğŸ”… Datasetì´ ê°™ì€ ì–¼êµ´ ì‚¬ëŒ ì‚¬ì§„ì´ trainê³¼ validì— ë“¤ì–´ê°€ë©´ validì—ì„œ í•™ìŠµí•œ ì‚¬ëŒì˜ ì–¼êµ´ë¡œ testí•˜ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ë¯€ë¡œ ì‚¬ëŒìœ¼ë¡œ train/val ë‚˜ëˆ”
  
   
