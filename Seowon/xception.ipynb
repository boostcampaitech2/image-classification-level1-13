{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-foster",
   "metadata": {
    "id": "geographic-foster"
   },
   "source": [
    "# Lesson 4 - Data Generation\n",
    "- 이번 실습자료에서는 파이토치 모델에 이미지를 입력값으로 주기위해 전처리를 하는 방법을 배웁니다.\n",
    "- 파이토치는 torch.utils.data에 있는 Dataset, DataLoader 클래스가 이 작업을 간편하게 해줍니다.\n",
    "## 0. Libraries & Configurations\n",
    "- 시각화에 필요한 라이브러리와 데이터 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "occasional-boxing",
   "metadata": {
    "id": "occasional-boxing"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8d2c989-10ec-40e0-9fb3-39cfe0c7dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "complex-israel",
   "metadata": {
    "id": "complex-israel"
   },
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "whole-computer",
   "metadata": {
    "id": "whole-computer"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-neighborhood",
   "metadata": {
    "id": "minute-neighborhood"
   },
   "source": [
    "## 1. Image Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-protest",
   "metadata": {
    "id": "christian-protest"
   },
   "source": [
    "- 2강 실습자료에서 사용되었던 데이터셋의 RGB 평균, 표준편차를 구하는 함수를 이용하여 그에 대해 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "crude-frank",
   "metadata": {
    "id": "crude-frank"
   },
   "outputs": [],
   "source": [
    "# def get_ext(img_dir, img_id):\n",
    "#     filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
    "#     ext = os.path.splitext(filename)[-1].lower()\n",
    "#     return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "superb-profession",
   "metadata": {
    "id": "superb-profession"
   },
   "outputs": [],
   "source": [
    "# def get_img_stats(img_dir, img_ids):\n",
    "#     img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "#     for img_id in tqdm(img_ids):\n",
    "#         for path in glob(os.path.join(img_dir, img_id, '*')):\n",
    "#             img = np.array(Image.open(path))\n",
    "#             h, w, _ = img.shape\n",
    "#             img_info['heights'].append(h)\n",
    "#             img_info['widths'].append(w)\n",
    "#             img_info['means'].append(img.mean(axis=(0,1)))\n",
    "#             img_info['stds'].append(img.std(axis=(0,1)))\n",
    "#     return img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "undefined-patrol",
   "metadata": {
    "id": "undefined-patrol",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_info = get_img_stats(img_dir, df.path.values)\n",
    "\n",
    "# print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "# print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-operation",
   "metadata": {
    "id": "compliant-operation"
   },
   "source": [
    "## 2. Dataset\n",
    "- 이 부분에서는 Dataset을 정의하는 방법을 간단하게 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-juvenile",
   "metadata": {
    "id": "democratic-juvenile"
   },
   "source": [
    "## 2.1 Augmentation Function\n",
    "- 3강에서 배운 Augmentation 함수를 정의합니다.\n",
    "- mean, std는 임의로 설정하였으나 파트 1에서 계산한 값을 입력해도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "horizontal-strain",
   "metadata": {
    "id": "horizontal-strain"
   },
   "outputs": [],
   "source": [
    "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-recycling",
   "metadata": {
    "id": "distant-recycling"
   },
   "source": [
    "### 2.1.1 Torchvision Style Augmentation Function\n",
    "- Torchvision에서 제공되는 transforms를 이용한 Augmentation 함수입니다.\n",
    "- 이를 사용하여 Dataset을 정의하여도 괜찮지만, 이번 실습자료에서는 강의에서 배웠던 Albumentation을 활용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "current-hometown",
   "metadata": {
    "id": "current-hometown",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Torchvision-Style Transforms\\nfrom torchvision import transforms\\nfrom torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur, RandomRotation, ColorJitter\\n\\n\\nclass AddGaussianNoise(object):\\n    def __init__(self, mean=0., std=1.):\\n        self.std = std\\n        self.mean = mean\\n\\n    def __call__(self, tensor):\\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\\n\\n    def __repr__(self):\\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\\n\\n\\ndef get_transforms(need=('train', 'val'), img_size=(512, 384)):\\n    transformations = {}\\n    if 'train' in need:\\n        transformations['train'] = transforms.Compose([\\n            Resize((img_size[0], img_size[1])),\\n            RandomRotation([-8, +8]),\\n            GaussianBlur(51, (0.1, 2.0)),\\n            ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),  # todo : param\\n            ToTensor(),\\n            Normalize(mean=mean, std=std),\\n            AddGaussianNoise(0., 1.)\\n        ])\\n    if 'val' in need:\\n        transformations['val'] = transforms.Compose([\\n            Resize((img_size[0], img_size[1])),\\n            ToTensor(),\\n            Normalize(mean=mean, std=std),\\n        ])\\n    return transformations\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Torchvision-Style Transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur, RandomRotation, ColorJitter\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384)):\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            RandomRotation([-8, +8]),\n",
    "            GaussianBlur(51, (0.1, 2.0)),\n",
    "            ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),  # todo : param\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            AddGaussianNoise(0., 1.)\n",
    "        ])\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = transforms.Compose([\n",
    "            Resize((img_size[0], img_size[1])),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "    return transformations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-survivor",
   "metadata": {
    "id": "sudden-survivor"
   },
   "source": [
    "### 2.1.2 Albumentation Style Augmentation Function\n",
    "- Albumentation은 numpy 형식으로 이미지를 받아 데이터를 변형시킵니다.\n",
    "- opencv 기반으로 빠르고, 다양한 Augmentation 방법이 제공되는 점에서 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "national-diameter",
   "metadata": {
    "id": "national-diameter"
   },
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             ShiftScaleRotate(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             GaussNoise(p=0.5),\n",
    "            CenterCrop(300, 200, p=1.0),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            CenterCrop(300, 200, p=1.0),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-lafayette",
   "metadata": {
    "id": "partial-lafayette"
   },
   "source": [
    "### 2.2 Define Dataset\n",
    "\n",
    "- 여기에서는 이미지와 레이블을 출력하는 Dataset 클래스를 정의합니다.\n",
    "- 레이블은 마스크 여부, 성별, 나이로 결정이 됩니다.\n",
    "- 레이블은 3(마스크 착용, 미착용, 잘못착용) * 2(남성, 여성) * 3(30세 미만, 30세-60세, 60세 이상) 으로 총 18개가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "annoying-emission",
   "metadata": {
    "id": "annoying-emission"
   },
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "divine-transmission",
   "metadata": {
    "id": "divine-transmission"
   },
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "secure-plasma",
   "metadata": {
    "id": "secure-plasma"
   },
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir\n",
    ")\n",
    "\n",
    "# train dataset과 validation dataset을 95:5 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.1)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-membrane",
   "metadata": {
    "id": "regulation-membrane"
   },
   "source": [
    "## 3. DataLoader\n",
    "- 정의한 Dataset을 바탕으로 DataLoader을 생성합니다.\n",
    "- Dataset은 이미지 한장을 주는 모듈이라면, DataLoader은 여러 이미지를 batch_size만큼 묶어 전달해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "blessed-robert",
   "metadata": {
    "id": "blessed-robert"
   },
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-richardson",
   "metadata": {
    "id": "tracked-richardson"
   },
   "source": [
    "## 4.  Visualize Processed Data\n",
    "- 파트 4에선 정의한 DataLoader을 이용하여 데이터가 어떻게 전처리 되었는지 시각화하여 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "explicit-bathroom",
   "metadata": {
    "id": "explicit-bathroom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([64, 3, 300, 200])\n",
      "labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(f'images shape: {images.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-reynolds",
   "metadata": {
    "id": "several-reynolds"
   },
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# # Augmentation으로 이미지를 Normalize했기 때문에, 역으로 다시 Normalize 해주어야합니다.\n",
    "# inv_normalize = transforms.Normalize(\n",
    "#     mean=[-m / s for m, s in zip(mean, std)],\n",
    "#     std=[1 / s for s in std]\n",
    "# )\n",
    "\n",
    "# n_rows, n_cols = 4, 3\n",
    "\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
    "# for i in range(n_rows*n_cols):\n",
    "#     axes[i%n_rows][i//(n_cols+1)].imshow(inv_normalize(images[i]).permute(1, 2, 0))\n",
    "#     axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1c6c5-7554-4ab1-a437-6a04b0b3ec22",
   "metadata": {},
   "source": [
    "### <i>여기부터 추가</i>\n",
    "## 5. Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24b4e887-66aa-4045-9a6f-6754e51bc67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/lib/python3.8/site-packages (0.4.12)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.10.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (1.19.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (8.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91cc5f32-4f7e-441d-8379-c0a850390883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)\n",
    "# model.fc = nn.Linear(2048, 18)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fe371a3-4368-4aa2-a9cf-42ad2a10e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import torch\n",
    "\n",
    "model = timm.create_model('xception', pretrained=True, num_classes=18)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "986a121f-088a-4985-b8c5-6d8a31e168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "dtype = torch.float\n",
    "ltype = torch.long # entropy\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e21f948-3ffb-4f5a-8dfe-678b84e5425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c953a4-add4-4c44-9162-b5ed8e1efe32",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a93288b7-1d93-41ee-937e-c3b4dd59b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.0028 | Classifier Accuracy 100.00\n",
      "Iteration  20 | Train Loss  0.0042 | Classifier Accuracy 100.00\n",
      "Iteration  40 | Train Loss  0.0035 | Classifier Accuracy 100.00\n",
      "Iteration  60 | Train Loss  0.0073 | Classifier Accuracy 100.00\n",
      "Iteration  80 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0070 | Classifier Accuracy 100.00\n",
      "Iteration 120 | Train Loss  0.0039 | Classifier Accuracy 100.00\n",
      "Iteration 140 | Train Loss  0.0068 | Classifier Accuracy 100.00\n",
      "Iteration 160 | Train Loss  0.0118 | Classifier Accuracy 100.00\n",
      "Iteration 180 | Train Loss  0.0036 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0036 | Classifier Accuracy 100.00\n",
      "Iteration 220 | Train Loss  0.0035 | Classifier Accuracy 100.00\n",
      "Iteration 240 | Train Loss  0.0033 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 36 s\n",
      "Train Loss Mean 0.0052 | Accuracy 99.83 \n",
      "Valid Loss Mean 0.0473 | Accuracy 95.58 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 3\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for i, (images, targets) in enumerate(train_loader) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device, dtype)\n",
    "        targets = targets.to(device, ltype)\n",
    "\n",
    "        scores = model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/64 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    model.eval()\n",
    "    for i, (images, targets) in enumerate(val_loader) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device, dtype=dtype)\n",
    "        targets = targets.to(device=device, dtype=ltype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/64 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        best_model = model\n",
    "\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d17de209-94be-4213-a98c-c8f1f4afdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ee3c1d7-ba04-4dcf-b41d-cf65d7e20118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "# transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "# dataset = MaskBaseDataset(\n",
    "#     img_dir=img_dir\n",
    "# )\n",
    "\n",
    "# # train dataset과 validation dataset을 9:1 비율로 나눕니다.\n",
    "# n_val = int(len(dataset) * 0.1)\n",
    "# n_train = len(dataset) - n_val\n",
    "# train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# # 각 dataset에 augmentation 함수를 설정합니다.\n",
    "# train_dataset.dataset.set_transform(transform['train'])\n",
    "# val_dataset.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d66e37f2-5b82-491b-a7cb-5ca86bb69f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, CenterCrop\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop((300, 200)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "# transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "# dataset = MaskBaseDataset(\n",
    "#     img_dir=test_dir\n",
    "# )\n",
    "\n",
    "# dataset.dataset.set_transform(transform['val'])\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = best_model\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device=device, dtype=dtype)\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, '0828_submission_xception2.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_DataGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
