{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac233a5-3ffd-4be4-bb45-0a25394b84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import sys\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize, ToTensor, CenterCrop, Normalize, GaussianBlur, RandomRotation, ColorJitter, RandomHorizontalFlip, RandomResizedCrop\n",
    "from sklearn.metrics import f1_score\n",
    "#난수 시드 설정\n",
    "# torch.manual_seed(1234)\n",
    "# np.random.seed(1234)\n",
    "# random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565f09ad-6c6d-4b71-8724-dc3de5f18e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:[cuda:0].\n",
      "CUDA: True\n",
      "PyTorch Version :  1.7.1\n",
      "torch Version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch Version : \", torch.__version__)\n",
    "print(f\"torch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a511b2b-a82a-4cc5-a6f3-a1f62c2101d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.deterministic= True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e5dc87-bcae-40c3-a37d-b6ec952c1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786a2132-d6c1-4631-9d1c-4b13b6049095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5\n",
    "\n",
    "# fig, ax = plt.subplots(7,N)\n",
    "# fig.set_figheight(30)\n",
    "# fig.set_figwidth(15)\n",
    "\n",
    "# for j, n in enumerate(np.random.randint(0,len(df),size = N)):\n",
    "#     path = \"/opt/ml/input/data/train/images/\" + df[\"path\"][n]\n",
    "#     i = 0\n",
    "#     for im_name in os.listdir(path):\n",
    "#         if not im_name.startswith(\".\"):\n",
    "#             im = Image.open(path + '/' + im_name)\n",
    "\n",
    "#             ax[i][j].set_title(im_name)\n",
    "#             ax[i][j].imshow(im)\n",
    "#             ax[i][j].axis(\"off\")\n",
    "#             i += 1\n",
    "        \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2ec383-0c3b-4fa6-82f8-90c51c8dc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_avg = np.zeros((512,384,3),dtype = np.float32)\n",
    "# for n in range(len(df)):\n",
    "#     path = \"/opt/ml/input/data/train/images/\" + df[\"path\"][n]\n",
    "#     for im_name in os.listdir(path):\n",
    "#         if not im_name.startswith(\".\"):\n",
    "#             im_avg += np.array(Image.open(path + '/' + im_name),dtype = np.float32)/225\n",
    "            \n",
    "# im_avg = im_avg/(len(df)*7)\n",
    "# fig, ax = plt.subplots(1,1, figsize = (6, 4))\n",
    "# ax.imshow(im_avg)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d7a9ff-c2dc-4d65-82c8-d6fce1b3ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.patches as patches\n",
    "# x_1, y_1 = 100, 100\n",
    "# x_2, y_2 = 300, 400\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# ax.imshow(im_avg)\n",
    "\n",
    "# # Create a Rectangle patch\n",
    "# rect = patches.Rectangle(\n",
    "#     xy       = (80, 50),\n",
    "#     width    = 220,\n",
    "#     height   = 320,\n",
    "#     linewidth= 1,\n",
    "#     edgecolor= 'r',\n",
    "#     facecolor= 'none'\n",
    "# )\n",
    "\n",
    "# # Add the patch to the Axes\n",
    "# ax.add_patch(rect)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4c7f21-16d1-48f4-b6c7-54985eab6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "def crop800(image):\n",
    "    return crop(image, 80, 50, 370, 300)\n",
    "\n",
    "# x_1, y_1 = 100, 100\n",
    "# x_2, y_2 = 300, 400\n",
    "# fig, ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# ax.imshow(crop800(im_avg))\n",
    "\n",
    "# data_transforms = {\n",
    "#     'images': transforms.Compose([transforms.ToTensor(),\n",
    "#                                   transforms.Lambda(crop800),\n",
    "#                                   transforms.Resize((400, 400))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc97e5f-38de-4266-b96f-f04d8187f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageTransform():\n",
    "    def __init__(self, resize, mean, std,phase=('train','value')):\n",
    "        transformations = {}\n",
    "        self.data_transform = {\n",
    "            'train' : transforms.Compose([\n",
    "#                 transforms.RandomResizedCrop(resize, scale = (0.5,1.0)), #데이터 확장\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation([-2, +2]),\n",
    "                transforms.GaussianBlur(51, (0.1, 2.0)),\n",
    "                transforms.ColorJitter(brightness=0.9, saturation=0.5, hue=0.5),  # todo : param\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(crop800),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]),\n",
    "            'val' : transforms.Compose([\n",
    "                transforms.Resize(resize),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "        }\n",
    "    def __call__(self, img, phase='train'):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "835bddbd-9c53-45c6-b439-7b63db1fd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 화상 전처리 확인\n",
    "\n",
    "# #1. 화상 읽기\n",
    "# image_file_path = '/opt/ml/input/data/train/images/000001_female_Asian_45/mask1.jpg'\n",
    "# img = Image.open(image_file_path) #[높이][너비][색RGB]\n",
    "# #2. 원본 화상 표시\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "# #3. 화상 전처리 및 처리된 화상의 표시\n",
    "size = 300\n",
    "mean = (0.560, 0.524, 0.501)\n",
    "std = (0.233, 0.243, 0.245)\n",
    "transform = ImageTransform(size, mean, std)\n",
    "# img_transformed = transform(img, phase=\"train\") #torch.Size([3, 300, 300])\n",
    "# # # (색상, 높이, 너비)를 (높이, 너비, 색상)으로 변환하고 0-1 로 값을 제한하여 표시\n",
    "# img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n",
    "# img_transformed = np.clip(img_transformed, 0, 1)\n",
    "# plt.imshow(img_transformed)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc75084f-8d9f-46a6-ba57-955ab4c243fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5baf1ac-429a-44e2-ae1b-05373585d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HymenopterDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "        self.transform = transform\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image)\n",
    "        return image_transform, multi_class_label\n",
    "        \n",
    "        if not self.split_labels:\n",
    "            multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "            return image_transform, multi_class_label\n",
    "        else:\n",
    "            return image_transform, (mask_label, gender_label, age_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dd66372-1e5c-47a8-897d-7fb8cd578e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = '/opt/ml/input/data/train/images'\n",
    "val_img_dir = '/opt/ml/input/data/eval/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de97c6e1-ce8b-4631-bc1f-66bee9ed5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "dataset =HymenopterDataset(img_dir=train_img_dir)\n",
    "\n",
    "n_val = int(len(dataset) * 0.1)\n",
    "n_train = len(dataset) - n_val\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                           [n_train, n_val])\n",
    "train_transform =ImageTransform(size, mean, std, phase = 'train')\n",
    "val_transform = ImageTransform(size, mean, std, phase = 'val')\n",
    "# val_dataset = transform(transform['val'])\n",
    "\n",
    "train_dataset = HymenopterDataset(img_dir = train_img_dir, transform = train_transform)\n",
    "\n",
    "val_dataset = HymenopterDataset(img_dir = val_img_dir, transform = val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc5bebb-5f46-4b4b-8e75-925f489dc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 370, 300])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 동작 확인\n",
    "index = 0\n",
    "print(train_dataset.__getitem__(index)[0].size()) #Chanel:3 width: 300 height :300\n",
    "print(train_dataset.__getitem__(index)[1]) # mask, incorrect_mask, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5dec1-d48a-4a17-92c8-aa6d9410af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#미니 배치 크기 지정:\n",
    "batch_size = 32\n",
    "\n",
    "#데이터 로더 작성\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size = batch_size, shuffle = True, num_workers = 2, pin_memory = True)\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# 사전형 변수에 정리\n",
    "dataloaders_dict = {\"train\" : train_dataloader, \"val\" : val_dataloader}\n",
    "\n",
    "#동작 확인\n",
    "batch_iterator = iter(dataloaders_dict[\"train\"])\n",
    "inputs, labels = next(\n",
    "    batch_iterator)\n",
    "print(inputs.size())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cadbb-c463-4770-a859-6d1286fba0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습된 VGG-16 모델 로드\n",
    "# VGG-16 모델의 인스턴스 생성\n",
    "use_pretrained = True #학습된 파라미터 사용\n",
    "net = models.vgg16(pretrained = use_pretrained)\n",
    "\n",
    "#VGG-16 마지막 출력층의 출력 유닛을 18개의 클래스로 변경\n",
    "net.classifier[6] = nn.Linear(in_features = 1000, out_features = 18)\n",
    "#훈련 모드로 설정\n",
    "net.train()\n",
    "print('네트워크 설정 완료 : 학습된 가중치를 읽어들여 훈련 모드로 설정했습니다.')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cfda5-e2e9-4e4c-914f-60daea5e9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 전이학습에서 학습시킬 파라미터를 params_to_update 변수에 저장\n",
    "params_to_update_1 = []\n",
    "params_to_update_2 = []\n",
    "params_to_update_3 = []\n",
    "\n",
    "\n",
    "#학습시킬 파라미터명\n",
    "update_param_names_1 = [\"features\"]\n",
    "update_param_names_2 = [\"classifier.0.weight\",\n",
    "                        \"classifier.0.biase\", \"classifier.3.weight\",\n",
    "                        \"classifier.3.bias\"]\n",
    "update_param_names_3 = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "#학습시킬 파라미터 외에는 경사를 계산하지 않고 변하지 않도록 설정\n",
    "for name, param in net.named_parameters():\n",
    "    if update_param_names_1[0] in name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_1.append(param)\n",
    "        print(\"params_to_update_1에 저장: \", name)\n",
    "    elif name in update_param_names_2:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_2.append(param)\n",
    "        print(\"params_to_update_2에 저장: \", name)        \n",
    "    elif name in update_param_names_3:\n",
    "        param.requires_grad = True\n",
    "        params_to_update_3.append(param)\n",
    "        print(\"params_to_update_3에 저장: \", name) \n",
    "    else:\n",
    "        param.requires_grad =False\n",
    "        print(\"경사 계산 없음. 학습하지 않음: \", name)\n",
    "\n",
    "#최적화 방법 설정\n",
    "optimizer = optim.SGD([\n",
    "    {'params' : params_to_update_1, 'lr' :1e-4},\n",
    "    {'params' : params_to_update_2, 'lr' :5e-4},\n",
    "    {'params' : params_to_update_3, 'lr' :1e-3}\n",
    "], momentum = 0.9, weight_decay = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98abf6-abda-4c5c-8086-de72d1bbf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e6807-bef9-4063-b86f-49db998f3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# config = dict(learning_rate =0.01,\n",
    "#               momentum = 0.9,\n",
    "#               architecture = \"VGG16\",\n",
    "#               dataset_id = \"MarkBaseDataset\",)\n",
    "# run = wandb.init(\n",
    "#     project = \"singon\",\n",
    "#     name = \"my_test_10\",\n",
    "#     notes = \"base_line_test\",\n",
    "#     tags = [\"Holiday\"],\n",
    "#     config = config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f62b9d9-df1c-4570-b26f-46b6afabcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  2.9310 | Classifier Accuracy 3.12\n",
      "Iteration  20 | Train Loss  2.3175 | Classifier Accuracy 28.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e3b506ae6386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "#네트워크를 GPU로\n",
    "dtype = torch.float\n",
    "ltype = torch.long\n",
    "net.to(device)\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "num_epochs = 1\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    epoch_f1 = 0\n",
    "    n_iter = 0\n",
    "    # train\n",
    "    net.train()\n",
    "    for i, (images, targets) in enumerate(train_dataloader) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device, dtype)\n",
    "        targets = targets.to(device, ltype)\n",
    "\n",
    "        scores = net(images)\n",
    "        _, preds = torch.max(scores,1)\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "        del images, targets\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "#     wandb.log({\"train_average accuracy\" : train_mean_acc})\n",
    "#     wandb.log({\"train_average_loss\" : train_mean_loss})    \n",
    "\n",
    "    \n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "\n",
    "    net.eval()\n",
    "    for i, (images, targets) in enumerate(val_dataloader) : \n",
    "        \n",
    "        images = images.to(device=device, dtype=dtype)\n",
    "        targets = targets.to(device=device, dtype=ltype)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = net(images)\n",
    "            _, preds = torch.max(scores,1)\n",
    "            loss = criterion(scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_f1 += f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        n_iter += 1\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "        \n",
    "        if (i+1) % 20 == 0 :\n",
    "            print(f'Iteration %3.d | validation Loss  %.4f | Class Acc(validation) %2.2f' % (i+1, loss, acc))\n",
    "        del images, targets\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    " \n",
    "\n",
    "    print(f'average valid loss %.4f | average valid accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "    \n",
    "    epoch_f1 = epoch_f1/n_iter\n",
    "    print(f\"{epoch_f1:.4f}\")\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        best_model = net\n",
    "#         wandb.log({\"average validation accuracy\" : val_mean_acc})\n",
    "#         wandb.log({\"average validation loss\" : val_mean_loss})\n",
    "\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b921fe-2d02-4ced-a688-871f6f5792cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a18cda-a5bf-41e9-812a-287efda1c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, CenterCrop\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop((300, 200)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "# transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "# dataset = MaskBaseDataset(\n",
    "#     img_dir=test_dir\n",
    "# )\n",
    "\n",
    "# dataset.dataset.set_transform(transform['val'])\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "net = best_model\n",
    "net.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device=device, dtype=dtype)\n",
    "        images = images.to(device)\n",
    "        pred = net(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, '0828_submission_VGG16.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdc599-57bc-41cd-b9e5-978a7c4af31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
