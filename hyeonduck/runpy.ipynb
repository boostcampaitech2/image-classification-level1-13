{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24917,
     "status": "ok",
     "timestamp": 1630293803468,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "mIg0yvPACVZH",
    "outputId": "818549f2-54c2-46e2-f31a-87a17cd33950"
   },
   "outputs": [],
   "source": [
    "# google drive import\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9533,
     "status": "ok",
     "timestamp": 1630283721167,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "2Gzoni_aCaP9",
    "outputId": "ac073834-933f-41d1-8c65-de47d9a0bdf4"
   },
   "outputs": [],
   "source": [
    "##dotenv 설치\n",
    "#!pip install dotenv-python\n",
    "#!pip install colab-env --upgrade\n",
    "#!pip install transformers\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1630283729592,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "_kh7IVYhjREM",
    "outputId": "817907a0-4454-4df2-f98c-8c142a3132d4"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YTNVmlzD8et"
   },
   "outputs": [],
   "source": [
    "#압축 풀기\n",
    "#!tar -xvf \"train.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1693,
     "status": "ok",
     "timestamp": 1630239924250,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "GwilKR6LHm4j",
    "outputId": "9fdda530-d6c9-4cdc-832a-976339733cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimson\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-snowball-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/simson/13AI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/simson/13AI/runs/3k3tq65w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /opt/ml/input/data/wandb/run-20210831_081655-3k3tq65w\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "usage: train.py [-h] [--seed SEED] [--epochs EPOCHS] [--dataset DATASET]\n",
      "                [--augmentation AUGMENTATION] [--resize RESIZE [RESIZE ...]]\n",
      "                [--batch_size BATCH_SIZE]\n",
      "                [--valid_batch_size VALID_BATCH_SIZE] [--model MODEL]\n",
      "                [--optimizer OPTIMIZER] [--lr LR] [--val_ratio VAL_RATIO]\n",
      "                [--criterion CRITERION] [--lr_decay_step LR_DECAY_STEP]\n",
      "                [--log_interval LOG_INTERVAL] [--name NAME]\n",
      "                [--data_dir DATA_DIR] [--model_dir MODEL_DIR]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --seed SEED           random seed (default: 42)\n",
      "  --epochs EPOCHS       number of epochs to train (default: 1)\n",
      "  --dataset DATASET     dataset augmentation type (default: MaskBaseDataset)\n",
      "  --augmentation AUGMENTATION\n",
      "                        data augmentation type (default: BaseAugmentation)\n",
      "  --resize RESIZE [RESIZE ...]\n",
      "                        resize size for image when training\n",
      "  --batch_size BATCH_SIZE\n",
      "                        input batch size for training (default: 64)\n",
      "  --valid_batch_size VALID_BATCH_SIZE\n",
      "                        input batch size for validing (default: 1000)\n",
      "  --model MODEL         model type (default: BaseModel)\n",
      "  --optimizer OPTIMIZER\n",
      "                        optimizer type (default: SGD)\n",
      "  --lr LR               learning rate (default: 1e-3)\n",
      "  --val_ratio VAL_RATIO\n",
      "                        ratio for validaton (default: 0.1)\n",
      "  --criterion CRITERION\n",
      "                        criterion type (default: cross_entropy)\n",
      "  --lr_decay_step LR_DECAY_STEP\n",
      "                        learning rate scheduler deacy step (default: 20)\n",
      "  --log_interval LOG_INTERVAL\n",
      "                        how many batches to wait before logging training\n",
      "                        status\n",
      "  --name NAME           model save at {SM_MODEL_DIR}/{name}\n",
      "  --data_dir DATA_DIR\n",
      "  --model_dir MODEL_DIR\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 17928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /opt/ml/input/data/wandb/run-20210831_081655-3k3tq65w/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /opt/ml/input/data/wandb/run-20210831_081655-3k3tq65w/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mworldly-snowball-2\u001b[0m: \u001b[34mhttps://wandb.ai/simson/13AI/runs/3k3tq65w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py  --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9322671,
     "status": "ok",
     "timestamp": 1630261529775,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "bSmdM8bCIdb3",
    "outputId": "be93d709-d003-45f3-81f3-a42055b13a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimson\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdenim-resonance-17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/simson/hd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/simson/hd/runs/lhjkk3qv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /opt/ml/input/data/wandb/run-20210831_132819-lhjkk3qv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Namespace(augmentation='BaseAugmentation', batch_size=32, criterion='cross_entropy', data_dir='train/images', dataset='MaskBaseDataset', epochs=100, log_interval=20, lr=0.001, lr_decay_step=20, model='resnext', model_dir='./model', name='exp', optimizer='Adam', resize=[112, 112], seed=42, val_ratio=0.1, valid_batch_size=1000)\n",
      "Epoch[0/100](20/472) || training loss 2.823 || training accuracy 17.19% || lr 0.001\n",
      "Epoch[0/100](40/472) || training loss 2.61 || training accuracy 20.62% || lr 0.001\n",
      "Epoch[0/100](60/472) || training loss 2.477 || training accuracy 23.59% || lr 0.001\n",
      "Epoch[0/100](80/472) || training loss 2.206 || training accuracy 21.88% || lr 0.001\n",
      "Epoch[0/100](100/472) || training loss 2.119 || training accuracy 24.38% || lr 0.001\n",
      "Epoch[0/100](120/472) || training loss 2.093 || training accuracy 25.16% || lr 0.001\n",
      "Epoch[0/100](140/472) || training loss 1.98 || training accuracy 25.62% || lr 0.001\n",
      "Epoch[0/100](160/472) || training loss 1.883 || training accuracy 28.91% || lr 0.001\n",
      "Epoch[0/100](180/472) || training loss 1.98 || training accuracy 27.97% || lr 0.001\n",
      "Epoch[0/100](200/472) || training loss 1.966 || training accuracy 26.56% || lr 0.001\n",
      "Epoch[0/100](220/472) || training loss 1.874 || training accuracy 29.69% || lr 0.001\n",
      "Epoch[0/100](240/472) || training loss 1.812 || training accuracy 34.06% || lr 0.001\n",
      "Epoch[0/100](260/472) || training loss 1.789 || training accuracy 34.22% || lr 0.001\n",
      "Epoch[0/100](280/472) || training loss 1.738 || training accuracy 36.41% || lr 0.001\n",
      "Epoch[0/100](300/472) || training loss 1.733 || training accuracy 40.94% || lr 0.001\n",
      "Epoch[0/100](320/472) || training loss 1.787 || training accuracy 35.47% || lr 0.001\n",
      "Epoch[0/100](340/472) || training loss 1.629 || training accuracy 42.50% || lr 0.001\n",
      "Epoch[0/100](360/472) || training loss 1.655 || training accuracy 46.72% || lr 0.001\n",
      "Epoch[0/100](380/472) || training loss 1.662 || training accuracy 42.50% || lr 0.001\n",
      "Epoch[0/100](400/472) || training loss 1.542 || training accuracy 45.62% || lr 0.001\n",
      "Epoch[0/100](420/472) || training loss 1.409 || training accuracy 52.50% || lr 0.001\n",
      "Epoch[0/100](440/472) || training loss 1.331 || training accuracy 57.66% || lr 0.001\n",
      "Epoch[0/100](460/472) || training loss 1.306 || training accuracy 56.25% || lr 0.001\n",
      "Calculating validation results...\n",
      "New best model for val accuracy : 45.21%! saving the best model..\n",
      "[Val] acc : 45.21%, loss:  1.3 || best acc : 45.21%, best loss:  1.3\n",
      "\n",
      "Epoch[1/100](20/472) || training loss 1.264 || training accuracy 57.03% || lr 0.001\n",
      "Epoch[1/100](40/472) || training loss 1.276 || training accuracy 58.91% || lr 0.001\n",
      "Epoch[1/100](60/472) || training loss 1.203 || training accuracy 57.34% || lr 0.001\n",
      "Epoch[1/100](80/472) || training loss 1.24 || training accuracy 60.47% || lr 0.001\n",
      "Epoch[1/100](100/472) || training loss 1.117 || training accuracy 65.94% || lr 0.001\n",
      "Epoch[1/100](120/472) || training loss 1.13 || training accuracy 60.78% || lr 0.001\n",
      "Epoch[1/100](140/472) || training loss 1.17 || training accuracy 60.94% || lr 0.001\n",
      "Epoch[1/100](160/472) || training loss 1.136 || training accuracy 61.56% || lr 0.001\n",
      "Epoch[1/100](180/472) || training loss 1.071 || training accuracy 62.50% || lr 0.001\n",
      "Epoch[1/100](200/472) || training loss 1.117 || training accuracy 61.56% || lr 0.001\n",
      "Epoch[1/100](220/472) || training loss 1.016 || training accuracy 65.94% || lr 0.001\n",
      "Epoch[1/100](240/472) || training loss 1.205 || training accuracy 60.94% || lr 0.001\n",
      "Epoch[1/100](260/472) || training loss 1.079 || training accuracy 63.12% || lr 0.001\n",
      "Epoch[1/100](280/472) || training loss 1.058 || training accuracy 61.72% || lr 0.001\n",
      "Epoch[1/100](300/472) || training loss 1.077 || training accuracy 65.31% || lr 0.001\n",
      "Epoch[1/100](320/472) || training loss 1.02 || training accuracy 65.78% || lr 0.001\n",
      "Epoch[1/100](340/472) || training loss 1.043 || training accuracy 65.47% || lr 0.001\n",
      "Epoch[1/100](360/472) || training loss 0.9988 || training accuracy 66.72% || lr 0.001\n",
      "Epoch[1/100](380/472) || training loss 1.017 || training accuracy 65.62% || lr 0.001\n",
      "Epoch[1/100](400/472) || training loss 0.981 || training accuracy 67.03% || lr 0.001\n",
      "Epoch[1/100](420/472) || training loss 0.9758 || training accuracy 66.41% || lr 0.001\n",
      "Epoch[1/100](440/472) || training loss 1.03 || training accuracy 66.09% || lr 0.001\n",
      "Epoch[1/100](460/472) || training loss 0.9473 || training accuracy 68.28% || lr 0.001\n",
      "Calculating validation results...\n",
      "New best model for val accuracy : 48.10%! saving the best model..\n",
      "[Val] acc : 48.10%, loss:  1.2 || best acc : 48.10%, best loss:  1.2\n",
      "\n",
      "Epoch[2/100](20/472) || training loss 0.9717 || training accuracy 69.06% || lr 0.001\n",
      "Epoch[2/100](40/472) || training loss 0.9681 || training accuracy 67.03% || lr 0.001\n",
      "Epoch[2/100](60/472) || training loss 0.8611 || training accuracy 69.53% || lr 0.001\n",
      "Epoch[2/100](80/472) || training loss 0.9127 || training accuracy 68.44% || lr 0.001\n",
      "Epoch[2/100](100/472) || training loss 0.9167 || training accuracy 69.06% || lr 0.001\n",
      "Epoch[2/100](120/472) || training loss 0.9148 || training accuracy 66.41% || lr 0.001\n",
      "Epoch[2/100](140/472) || training loss 0.8447 || training accuracy 73.59% || lr 0.001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python train.py  --data_dir train/images --model resnext --epochs 100 --optimizer Adam --batch_size 32\n",
    "'''\n",
    "!python train.py  --data_dir train/images --model resnext --epochs 100 --optimizer Adam --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1302,
     "status": "ok",
     "timestamp": 1630246710942,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "uCfgEODLO8-K",
    "outputId": "1a3811b1-f1a7-4615-f7cf-ffdc96a97459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: inference.py [-h] [--batch_size BATCH_SIZE] [--resize RESIZE]\n",
      "                    [--model MODEL] [--data_dir DATA_DIR]\n",
      "                    [--model_dir MODEL_DIR] [--output_dir OUTPUT_DIR]\n",
      "inference.py: error: argument -h/--help: ignored explicit argument 'elp'\n"
     ]
    }
   ],
   "source": [
    "!python inference.py -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 375789,
     "status": "ok",
     "timestamp": 1630286215398,
     "user": {
      "displayName": "kong king",
      "photoUrl": "",
      "userId": "00306219203767874913"
     },
     "user_tz": -540
    },
    "id": "Xe91UUS-Qf3D",
    "outputId": "e2ff2441-0a1e-4148-d52a-816959de90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"inference.py\", line 117, in <module>\n",
      "    inference(data_dir, model_dir, output_dir, args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"inference.py\", line 53, in inference\n",
      "    model = load_model(model_dir, num_classes, device).to(device)\n",
      "  File \"inference.py\", line 29, in load_model\n",
      "    model_cls = getattr(import_module(\"model\"), args.model)\n",
      "AttributeError: module 'model' has no attribute 'vit16'\n"
     ]
    }
   ],
   "source": [
    "!python inference.py   --model_dir model/exp  --model vit16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = '/opt/ml/input/data/train/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data/train'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPgyxEH04/vWdxA0bSdIkV3",
   "name": "runpy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
