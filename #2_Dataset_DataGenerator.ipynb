{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc92a5e-479f-4364-aa50-c963e95f54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aad5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, PILToTensor\n",
    "from torchvision.transforms import RandomCrop, RandomRotation, RandomHorizontalFlip\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba5da27-7d55-4831-a492-4b468935f0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30vnpv8j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15527<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/code/wandb/run-20210826_052003-30vnpv8j/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/code/wandb/run-20210826_052003-30vnpv8j/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>average accuracy</td><td>80.61691</td></tr><tr><td>_runtime</td><td>91</td></tr><tr><td>_timestamp</td><td>1629955294</td></tr><tr><td>_step</td><td>1</td></tr><tr><td>average loss</td><td>80.61691</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>average accuracy</td><td>▁</td></tr><tr><td>_runtime</td><td>▁▁</td></tr><tr><td>_timestamp</td><td>▁▁</td></tr><tr><td>_step</td><td>▁█</td></tr><tr><td>average loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">easy-darkness-11</strong>: <a href=\"https://wandb.ai/13ai/seunghun/runs/30vnpv8j\" target=\"_blank\">https://wandb.ai/13ai/seunghun/runs/30vnpv8j</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:30vnpv8j). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">silver-microwave-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/13ai/seunghun\" target=\"_blank\">https://wandb.ai/13ai/seunghun</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/13ai/seunghun/runs/m8tgsnrr\" target=\"_blank\">https://wandb.ai/13ai/seunghun/runs/m8tgsnrr</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210826_052520-m8tgsnrr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(m8tgsnrr)</h1><iframe src=\"https://wandb.ai/13ai/seunghun/runs/m8tgsnrr\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f43fed73d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Flexible integration for any Python script\n",
    "import wandb\n",
    "\n",
    "# 1. Start a W&B run\n",
    "wandb.init(project='seunghun', entity='13ai')\n",
    "\n",
    "# 2. Save model inputs and hyperparameters\n",
    "\n",
    "# Model training here\n",
    "\n",
    "# 3. Log metrics over time to visualize performance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2846fd1-47a8-4553-a680-75dc2d3f382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2\n",
    "\n",
    "    \n",
    "### Define Dataset\n",
    "class MaskBaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    1. label 이 (0, 1, 0) 과 같은 형식으로 나옴.\n",
    "    >>> MaskBaseDataset(img_dir, transform=transform, split_labels=True)\n",
    "    2. label 이 3 과 같은 형식으로 나옴.\n",
    "    >>> MaskBaseDataset(img_dir, transform=transform)\n",
    "    \"\"\"\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None, split_labels:bool=False):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "        self.mean = 0.5\n",
    "        self.std = 0.2\n",
    "        self.transform = transform\n",
    "        self.split_labels = split_labels\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "         - train_dataset, validation_dataset, test_dataset 이 정해지면 그 때 따로 설정한다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, mask_label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(mask_label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "\n",
    "        \n",
    "        if not self.split_labels:\n",
    "            multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "            return image_transform, multi_class_label\n",
    "        else:\n",
    "            return image_transform, (mask_label, gender_label, age_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a375a16-d682-4359-8596-7775e71579f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader\n",
    "\n",
    "RAW_ROW_SIZE = 512\n",
    "RAW_COL_SIZE = 384\n",
    "PROPORTION = 0.7\n",
    "\n",
    "ROW_SIZE = 128\n",
    "COL_SIZE = 128\n",
    "\n",
    "\n",
    "# random rotation & resize 적용한 train dataset 2\n",
    "train_transform = transforms.Compose([\n",
    "    RandomCrop(size=(int(RAW_ROW_SIZE*PROPORTION), \n",
    "                     int(RAW_COL_SIZE*PROPORTION))),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Resize((ROW_SIZE, COL_SIZE), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), \n",
    "              std=(0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    Resize((ROW_SIZE, COL_SIZE), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), \n",
    "              std=(0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "img_dir = os.path.join(train_dir, \"images\")\n",
    "\n",
    "dataset = MaskBaseDataset(img_dir=img_dir,\n",
    "                          split_labels=False)\n",
    "\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, \n",
    "                                                           [n_train, n_val])\n",
    "\n",
    "train_dataset.dataset.set_transform(train_transform)\n",
    "val_dataset.dataset.set_transform(val_transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                                batch_size=64,\n",
    "                                shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset,\n",
    "                                batch_size=64,\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b58f6fa-0eb5-4b18-890a-01e8bbfa9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "a_data = next(iter(train_data_loader))\n",
    "print(a_data[0].size())\n",
    "print(a_data[1].size())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a29c62-eafc-4852-ba81-d95a35d1a99c",
   "metadata": {},
   "source": [
    "class TrainDataset_w_1_feature(Dataset):\n",
    "    def __init__(self, train_csv_path, transform=None):\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.transform = transform\n",
    "        train_data = self.__set_target_mask_of_train(train_csv_path)\n",
    "        self.img_paths = train_data[\"img_path\"].to_numpy()\n",
    "        self.y = train_data[\"target_origin\"].to_numpy()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            result = (self.transform(Image.open(self.img_paths[index])), \n",
    "                      torch.tensor(self.y[index]))\n",
    "        else:\n",
    "            result = (PILToTensor(Image.open(self.img_paths[index])), torch.tensor(self.y[index]))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __is_not_hidden_file(self, file_name):\n",
    "        return file_name[0] != \".\"\n",
    "\n",
    "    def __set_target_mask_of_train(self, train_csv_path):\n",
    "        def cvt_age(age):\n",
    "            if age < 30:\n",
    "                return 0 # < 30\n",
    "            elif age < 60:\n",
    "                return 1 # 30 <= age < 60\n",
    "            else:\n",
    "                return 2 # >= 60\n",
    "\n",
    "        def cvt_gender(gender):\n",
    "            if gender.upper() == \"MALE\":\n",
    "                return 0 # male\n",
    "            else:\n",
    "                return 1 # female\n",
    "        \n",
    "        train_img_dir = os.path.join(train_dir, \"images\") # train_dir : global\n",
    "        \n",
    "        raw_train_data = pd.read_csv(train_csv_path)\n",
    "        returned_train = pd.DataFrame()\n",
    "        raw_train_data2np_array = np.array(raw_train_data)\n",
    "\n",
    "        idxes = []\n",
    "        genders = []\n",
    "        ages = []\n",
    "        target_masks = []\n",
    "        img_paths = []\n",
    "\n",
    "        for idx, gender, _, age, path in raw_train_data2np_array:\n",
    "            # 1. 데이터 1개에 대해 이미지가 있는 path를 지정\n",
    "            img_dir_path = os.path.join(train_img_dir, path)\n",
    "\n",
    "            for img_file_name in os.listdir(img_dir_path):\n",
    "                if self.__is_not_hidden_file(img_file_name):\n",
    "                    if \"incorrect\" in img_file_name:\n",
    "                        target_masks.append(1)\n",
    "                    elif \"mask\" in img_file_name:\n",
    "                        target_masks.append(0)\n",
    "                    elif \"normal\" in img_file_name:\n",
    "                        target_masks.append(2)\n",
    "\n",
    "                    idxes.append(idx)\n",
    "                    genders.append(gender)\n",
    "                    ages.append(age)\n",
    "\n",
    "                    img_path = os.path.join(img_dir_path, img_file_name)\n",
    "                    img_paths.append(img_path)\n",
    "\n",
    "        returned_train = pd.DataFrame([idxes, genders, ages, img_paths, target_masks]).T\n",
    "        returned_train.columns = [\"id\", \"gender\", \"age\", \"img_path\", \"target_mask\"]\n",
    "\n",
    "        returned_train[\"target_gender\"] = returned_train[\"gender\"].apply(cvt_gender)\n",
    "        returned_train[\"target_age\"] = returned_train[\"age\"].apply(cvt_age)\n",
    "\n",
    "        returned_train[\"target_origin\"] = 6 * returned_train[\"target_mask\"] + 3 * returned_train[\"target_gender\"] +  returned_train[\"target_age\"]\n",
    "\n",
    "        returned_train.drop([\"age\", \"gender\", \"target_mask\", \"target_gender\", \"target_age\", \"id\"], axis=1, inplace=True)\n",
    "\n",
    "        return returned_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a04d0dc3-f509-4e50-b4dd-3b51620a61f6",
   "metadata": {},
   "source": [
    "class TrainDataset_w_3_features(Dataset):\n",
    "    \"\"\"\n",
    "    target_mask   : wear(0), incorrect(1), normal(2)\n",
    "    target_gender : male(0), female(1)\n",
    "    target_age    : 0(<30), 1(>=30 & <60), 2(>=60)\n",
    "    \"\"\"\n",
    "    def __init__(self, train_csv_path, transform=None):\n",
    "        self.train_csv_path = train_csv_path\n",
    "        self.transform = transform\n",
    "        train_data = self.__set_target_mask_of_train(train_csv_path)\n",
    "        self.img_paths = train_data[\"img_path\"].to_numpy()\n",
    "        self.y = train_data[[\"target_mask\", \"target_gender\", \"target_age\"]].to_numpy()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            result = (self.transform(Image.open(self.img_paths[index])), torch.from_numpy(self.y[index]))\n",
    "        else:\n",
    "            result = (PILToTensor(Image.open(self.img_paths[index])), torch.from_numpy(self.y[index]))\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __is_not_hidden_file(self, file_name):\n",
    "        return file_name[0] != \".\"\n",
    "\n",
    "    def __set_target_mask_of_train(self, train_csv_path):\n",
    "        def cvt_age(age):\n",
    "            if age < 30:\n",
    "                return 0 # < 30\n",
    "            elif age < 60:\n",
    "                return 1 # 30 <= age < 60\n",
    "            else:\n",
    "                return 2 # >= 60\n",
    "\n",
    "        def cvt_gender(gender):\n",
    "            if gender.upper() == \"MALE\":\n",
    "                return 0 # male\n",
    "            else:\n",
    "                return 1 # female\n",
    "        \n",
    "        train_img_dir = os.path.join(train_dir, \"images\") # train_dir : global\n",
    "        \n",
    "        raw_train_data = pd.read_csv(train_csv_path)\n",
    "        returned_train = pd.DataFrame()\n",
    "        raw_train_data2np_array = np.array(raw_train_data)\n",
    "\n",
    "        idxes = []\n",
    "        genders = []\n",
    "        ages = []\n",
    "        target_masks = []\n",
    "        img_paths = []\n",
    "\n",
    "        for idx, gender, _, age, path in raw_train_data2np_array:\n",
    "            # 1. 데이터 1개에 대해 이미지가 있는 path를 지정\n",
    "            img_dir_path = os.path.join(train_img_dir, path)\n",
    "\n",
    "            for img_file_name in os.listdir(img_dir_path):\n",
    "                if self.__is_not_hidden_file(img_file_name):\n",
    "                    if \"incorrect\" in img_file_name:\n",
    "                        target_masks.append(1)\n",
    "                    elif \"mask\" in img_file_name:\n",
    "                        target_masks.append(0)\n",
    "                    elif \"normal\" in img_file_name:\n",
    "                        target_masks.append(2)\n",
    "\n",
    "                    idxes.append(idx)\n",
    "                    genders.append(gender)\n",
    "                    ages.append(age)\n",
    "\n",
    "                    img_path = os.path.join(img_dir_path, img_file_name)\n",
    "                    img_paths.append(img_path)\n",
    "\n",
    "        returned_train = pd.DataFrame([idxes, genders, ages, img_paths, target_masks]).T\n",
    "        returned_train.columns = [\"id\", \"gender\", \"age\", \"img_path\", \"target_mask\"]\n",
    "\n",
    "        returned_train[\"target_gender\"] = returned_train[\"gender\"].apply(cvt_gender)\n",
    "        returned_train[\"target_age\"] = returned_train[\"age\"].apply(cvt_age)\n",
    "        returned_train.drop([\"id\", \"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "        return returned_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "509f19d4-b3ce-49ed-9653-c3c4f7ebdcb8",
   "metadata": {},
   "source": [
    "RAW_ROW_SIZE = 512\n",
    "RAW_COL_SIZE = 384\n",
    "PROPORTION = 0.7\n",
    "\n",
    "ROW_SIZE = 128\n",
    "COL_SIZE = 128\n",
    "\n",
    "# train_dataset_normal = []\n",
    "# # resize만 적용한 train dataset 1\n",
    "# transform_normal = transforms.Compose([\n",
    "#     Resize((ROW_SIZE, COL_SIZE)),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "# ])\n",
    "\n",
    "# train_dataset_normal = TrainDataset_w_1_feature(os.path.join(train_dir, \"train.csv\"), \n",
    "#                                                 transform=transform_normal)\n",
    "\n",
    "# random rotation & resize 적용한 train dataset 2\n",
    "transform_rotate = transforms.Compose([\n",
    "    RandomRotation(degrees=(-15, 15)),\n",
    "    Resize((ROW_SIZE, COL_SIZE)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "train_dataset_rotate = TrainDataset_w_1_feature(os.path.join(train_dir, \"train.csv\"), \n",
    "                                                transform=transform_rotate)\n",
    "\n",
    "# random crop & resize 적용한 train dataset 3\n",
    "transform_crop = transforms.Compose([\n",
    "    RandomCrop(size=(int(RAW_ROW_SIZE*PROPORTION), int(RAW_COL_SIZE*PROPORTION))),\n",
    "    Resize((ROW_SIZE, COL_SIZE)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "train_dataset_crop = TrainDataset_w_1_feature(os.path.join(train_dir, \"train.csv\"), \n",
    "                                              transform=transform_crop)\n",
    "\n",
    "# train_dataset = train_dataset_normal + train_dataset_rotate + train_dataset_crop\n",
    "train_dataset = train_dataset_rotate + train_dataset_crop\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                                batch_size=64,\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "920e5c59-8e0a-4880-a08e-a020b435a03f",
   "metadata": {},
   "source": [
    "train_csv_path = os.path.join(train_dir, \"train.csv\")\n",
    "# my_train_dataset = TrainDataset(train_csv_path)\n",
    "iter_train_dataset = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dec4d92-5632-4efa-b06e-292b04b740d3",
   "metadata": {},
   "source": [
    "next(iter_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bd2c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 모델 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c98b12-2e33-4980-9f01-3d96d566c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Example\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 1000):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=64, \n",
    "#                       kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d0078e-7b17-4542-ab8f-95eda99e75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 18):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=48, \n",
    "#                       kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(48),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.maxpool((5, 5))\n",
    "#         )\n",
    "        \n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=48, out_channels=96, \n",
    "#                       kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(96),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.maxpool((3, 3))\n",
    "#         )\n",
    "        \n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=96, out_channels=48, \n",
    "#                       kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(48),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.maxpool((3, 3))\n",
    "#         )\n",
    "        \n",
    "# #         self.fc = nn.Sequential(\n",
    "# #             nn.\n",
    "# #         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49aaa882-7746-4716-a53c-adfd51d648d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "    \n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. 0 <= val <= 1\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "    \n",
    "    '''\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "    \n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "        \n",
    "    \n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2 * (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f159b",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration  20 | Train Loss  1.2391 | Class Acc(train) 65.62\n",
      "Iteration  40 | Train Loss  0.5432 | Class Acc(train) 78.12\n",
      "Iteration  60 | Train Loss  0.4129 | Class Acc(train) 84.38\n",
      "Iteration  80 | Train Loss  0.6504 | Class Acc(train) 71.88\n",
      "Iteration 100 | Train Loss  0.5043 | Class Acc(train) 84.38\n",
      "Iteration 120 | Train Loss  0.3235 | Class Acc(train) 90.62\n",
      "Iteration 140 | Train Loss  0.4268 | Class Acc(train) 84.38\n",
      "Iteration 160 | Train Loss  0.4416 | Class Acc(train) 78.12\n",
      "Iteration 180 | Train Loss  0.5444 | Class Acc(train) 84.38\n",
      "Iteration 200 | Train Loss  0.3394 | Class Acc(train) 84.38\n",
      "Iteration 220 | Train Loss  0.3057 | Class Acc(train) 85.94\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 33% |  5% |\n",
      "\n",
      "Iteration  20 | validation Loss  0.5975 | Class Acc(validation) 79.69 | f1 score 0.15\n",
      "Iteration  40 | validation Loss  0.7070 | Class Acc(validation) 76.56 | f1 score 0.13\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 19% |  5% |\n",
      "\n",
      " >>> average validation accuracy : 83.50\n",
      " >>>     average validation loss :  0.48\n",
      " >>> average validation f1 score :  0.14\n",
      "\n",
      " !!! Model Saved !!!\n",
      "     >>> accuracy : from -1.00 to 83.50\n",
      "     >>>     loss : from 10000000000.00 to 0.48\n",
      "     >>> f1 score : from -1.00 to 0.14\n",
      "\n",
      " ====================== epoch 2 ======================\n"
     ]
    }
   ],
   "source": [
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "# model = MyModel(num_classes=18).to(device)\n",
    "# model = CNN().to(device)\n",
    "\n",
    "# model = torch.load(\"./alexnet_before_fine_tune_210825_18h\").to(device)\n",
    "model = torch.load(\"./resnet18_before_fine_tune\").to(device)\n",
    "dtype = torch.float\n",
    "ltype = torch.long # entropy\n",
    "\n",
    "# model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "max_validation_acc = -1\n",
    "min_validation_loss = 1e+10\n",
    "max_f1_score = -1\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    ## Train\n",
    "    model.train()\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    avg_acc = 0.0\n",
    "    avg_loss = 0.0\n",
    "    \n",
    "#     for i, (images, targets) in tqdm(enumerate(train_data_loader)):\n",
    "    for i, (images, targets) in enumerate(train_data_loader): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device, dtype)\n",
    "        targets = targets.to(device, ltype)\n",
    "        scores = model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        avg_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc = (correct / len(targets) * 100)\n",
    "        avg_acc += acc\n",
    "        \n",
    "        if (i+1) % 20 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Class Acc(train) %2.2f' \n",
    "                  % (i+1, loss, acc))\n",
    "        \n",
    "        del images, targets\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_acc = avg_acc / (i+1)\n",
    "    avg_loss = avg_loss / (i+1)\n",
    "\n",
    "    wandb.log({\"average accuracy\" : avg_acc})\n",
    "    wandb.log({\"average loss\" : avg_acc})\n",
    "    \n",
    "    # train 이후 gpu memory 확인\n",
    "    gpu_usage()\n",
    "    print()\n",
    "    \n",
    "    ## Validation    \n",
    "    avg_validaion_acc = 0.0\n",
    "    avg_validaion_loss = 0.0\n",
    "    avg_valid_f1_score = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "#         for i, (images, targets) in tqdm(enumerate(val_data_loader)): \n",
    "        for i, (images, targets) in enumerate(val_data_loader):\n",
    "            images = images.to(device, dtype)\n",
    "            targets = targets.to(device, ltype)\n",
    "            scores = model(images)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            avg_validaion_loss += loss\n",
    "\n",
    "            correct = sum(targets == preds).cpu()\n",
    "            acc = (correct / len(targets) * 100)\n",
    "            avg_validaion_acc += acc\n",
    "            \n",
    "            valid_f1_score = f1_loss(y_true=targets, y_pred=preds) / len(targets)\n",
    "            avg_valid_f1_score += valid_f1_score\n",
    "            \n",
    "            if (i+1) % 20 == 0 :\n",
    "                print(f'Iteration %3.d | validation Loss  %.4f | Class Acc(validation) %2.2f | f1 score %2.2f' \n",
    "                      % (i+1, loss, acc, valid_f1_score))\n",
    "\n",
    "            del images, targets\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # valid 이후 gpu memory 확인\n",
    "    gpu_usage()\n",
    "    print()\n",
    "\n",
    "    avg_validaion_loss = avg_validaion_loss / (i+1)\n",
    "    avg_validaion_acc = avg_validaion_acc / (i+1)\n",
    "    avg_valid_f1_score = avg_valid_f1_score / (i+1)\n",
    "    \n",
    "    print(f\" >>> average validation accuracy : {avg_validaion_acc:5.2f}\")\n",
    "    print(f\" >>>     average validation loss : {avg_validaion_loss:5.2f}\")\n",
    "    print(f\" >>> average validation f1 score : {avg_valid_f1_score:5.2f}\")\n",
    "\n",
    "    # 성능 향상으로 보고 모델 저장\n",
    "    if avg_validaion_acc > max_validation_acc and avg_validaion_loss < min_validation_loss and avg_valid_f1_score > max_f1_score:\n",
    "        year = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).year\n",
    "        month = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).month\n",
    "        day = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).day\n",
    "        hour = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).hour\n",
    "        minute = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).minute\n",
    "        second = dt.datetime.now().astimezone(timezone(\"Asia/Seoul\")).second\n",
    "        \n",
    "        # 모델 세이브 파일 이름 컨벤션 논의!\n",
    "        print(\"\\n !!! Model Saved !!!\")\n",
    "        print(f\"     >>> accuracy : from {max_validation_acc:.2f} to {avg_validaion_acc:.2f}\")\n",
    "        print(f\"     >>>     loss : from {min_validation_loss:.2f} to {avg_validaion_loss:.2f}\")\n",
    "        print(f\"     >>> f1 score : from {max_f1_score:.2f} to {avg_valid_f1_score:.2f}\\n\")\n",
    "        torch.save(model, f\"./model_{e}_{int(avg_validaion_acc)}_{year}{month}{day}_{hour}h{minute}m{second}s\")\n",
    "        max_validation_acc, min_validation_loss, max_f1_score = avg_validaion_acc, avg_validaion_loss, avg_valid_f1_score\n",
    "        \n",
    "    wandb.log({\"average validation accuracy\" : avg_validaion_acc})\n",
    "    wandb.log({\"average validation loss\" : avg_validaion_loss})\n",
    "    wandb.log({\"average validation f1 score\" : avg_valid_f1_score})\n",
    "    \n",
    "    # early stop 기능\n",
    "    lr_sched.step()\n",
    "    \n",
    "#     torch.save(model, \"./model_backup\")\n",
    "# torch.save(model, f\"./model_{year}{month}{day}_{hour}h{minute}m{second}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c68c5e-c232-4466-81d4-d819504dc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # epoch 아래에 넣으면 early stop\n",
    "# EARLY_STOPPING_EPOCH = 5\n",
    "\n",
    "# if val_mean_loss < valid_best_loss:\n",
    "#     valid_best_loss = val_mean_loss\n",
    "#     valid_early_stop = 0\n",
    "#     # new best model save (valid 기준)\n",
    "#     best_model = model\n",
    "#     path = './model/'\n",
    "#     torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "# else:\n",
    "#     # early stopping    \n",
    "#     valid_early_stop += 1\n",
    "#     if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "#         print(\"EARLY STOPPING!!\")\n",
    "#         break\n",
    "\n",
    "# lr_sched.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a365c02-9692-4dc1-9489-c92a0462fa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<range_iterator at 0x7f65383430c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f06671-1a0b-4b14-b65a-0e9a7dca46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in tqdm(train_data_loader):\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d536e7-6b5d-41f6-98b5-be33576e30e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af569aab-1861-4e0e-92f6-c42992cbaa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10a99a-7c83-424f-b9bf-a71848d85da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
